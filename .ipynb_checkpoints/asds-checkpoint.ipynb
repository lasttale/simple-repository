{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "delayed-packing",
   "metadata": {},
   "source": [
    "# 영화리뷰 텍스트 감성분석하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-elephant",
   "metadata": {},
   "source": [
    "## 1) 데이터 준비와 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-productivity",
   "metadata": {},
   "source": [
    "## 2) 데이터로더 구성   \n",
    "\n",
    "    데이터의 중복 제거\n",
    "    NaN 결측치 제거\n",
    "    한국어 토크나이저로 토큰화\n",
    "    불용어(Stopwords) 제거\n",
    "    사전word_to_index 구성\n",
    "    텍스트 스트링을 사전 인덱스 스트링으로 변환\n",
    "    X_train, y_train, X_test, y_test, word_to_index 리턴\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    # [[YOUR CODE]]\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True) #중복제거\n",
    "    train_data = train_data.dropna(how = 'any') #제거로 인한 빈칸 지우기\n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = [] \n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화 : 문장을 분리시켜줌.\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist() #행렬 연결\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4) #10000개 단어를 쌓는데 아래 4개 자리를 비우기 위해 -4\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):#워드리스트에서 워드를 숫자로 변화시켜주는것.\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))#데이터 리스트 지정\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "#위에서 만든 함수를 아래 데이터 형태로 쪼개줌\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "#load_data를 쪼겐 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-moldova",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-cleveland",
   "metadata": {},
   "source": [
    "## 3) 모델구성을 위한 데이터 분석 및 가공   \n",
    "\n",
    "    데이터셋 내 문장 길이 분포\n",
    "    적절한 최대 문장 길이 지정\n",
    "    keras.preprocessing.sequence.pad_sequences 을 활용한 패딩 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_text = list(X_train) + list(X_test)\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) #수정 가능한 하이퍼파라미터\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index[\"<PAD>\"],\n",
    "                                                        padding='post', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index[\"<PAD>\"],\n",
    "                                                       padding='post', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-farmer",
   "metadata": {},
   "source": [
    "## 4) 모델구성 및 validation set 구성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 10000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "#word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "#model = keras.Sequential()\n",
    "#model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "#model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "#model.add(keras.layers.MaxPooling1D(5))\n",
    "#model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "#model.add(keras.layers.GlobalMaxPooling1D())\n",
    "#model.add(keras.layers.Dense(8, activation='relu'))\n",
    "#model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 10000  # 어휘 사전의 크기입니다(10개의 단어)\n",
    "#word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "#model = keras.Sequential()\n",
    "#model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "#model.add(keras.layers.GlobalMaxPooling1D())\n",
    "#model.add(keras.layers.Dense(8, activation='relu'))\n",
    "#model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(32))   # 벡터의 차원수 변경가능\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-visiting",
   "metadata": {},
   "source": [
    "## 5) 모델 훈련 개시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train[:10000]   \n",
    "y_val = y_train[:10000]\n",
    "\n",
    "\n",
    "partial_x_train = X_train[10000:]  \n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-election",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-motorcycle",
   "metadata": {},
   "source": [
    "1. Cov1D model 변경 후 InvalidArgumentError 발생 ---> vocab_size를 10000으로 word_vector_dim는 16으로 변경 후 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2) #테스트셋 평가\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-helicopter",
   "metadata": {},
   "source": [
    "## 6) Loss, Accuracy 그래프 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-inclusion",
   "metadata": {},
   "source": [
    "## 7) 학습된 Embedding 레이어 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(weights.shape)    # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 학습한 Embedding 파라미터를 파일에 써서 저장합니다. \n",
    "word2vec_file_path = os.getenv('HOME')+'/aiffel/sentiment_classification/word2vec.txt'\n",
    "f = open(word2vec_file_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim))  # 몇개의 벡터를 얼마 사이즈로 기재할지 타이틀을 씁니다.\n",
    "\n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록합니다. \n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "vector = word_vectors['강아지']\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-transparency",
   "metadata": {},
   "source": [
    "## 8) 한국어 Word2Vec 임베딩 활용하여 성능개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import KeyedVectors\n",
    "\n",
    "#word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=1000000)\n",
    "\n",
    "from gensim import models\n",
    "word2vec_path = os.getenv('HOME')+'/aiffel/sentiment_classification/ko/ko.bin'\n",
    "ko_model = models.Word2Vec.load(word2vec_path)\n",
    "vector = ko_model['강아지']\n",
    "vector     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-swift",
   "metadata": {},
   "source": [
    "1. 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte 오류 발생. ---> 모델 형식 변경 해결."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_model.similar_by_word(\"사람\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 ko-model 워드벡터를 단어 하나씩마다 차례차례 카피한다.\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in ko_model:\n",
    "        embedding_matrix[i] = ko_model[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-template",
   "metadata": {},
   "source": [
    "1. ValueError: could not broadcast input array from shape (200) into shape (300)\n",
    "발생 ----> word_vector_dim 을 200 으로 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 200  # 워드 벡터의 차원수 (변경가능한 하이퍼파라미터)\n",
    "\n",
    "# 모델 구성\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "#model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "#model.add(keras.layers.MaxPooling1D(5))\n",
    "#model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "#model.add(keras.layers.GlobalMaxPooling1D())\n",
    "#model.add(GRU(hidden_size, input_shape=(timesteps, input_dim)))\n",
    "model.add(keras.layers.LSTM(64))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-bridal",
   "metadata": {},
   "source": [
    " trainable를 False로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습의 진행\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=5  # 몇 epoch를 훈련하면 좋을지 결과를 보면서 바꾸어 봅시다. \n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-briefing",
   "metadata": {},
   "source": [
    "1. 1차시도 LSTM모델 epoch20 80.12 ---> epoch 4로 변경\n",
    "2. 2차시도 LSTM모델 epoch4  80.78 ---> 모델을 Conv1S로 변경\n",
    "3. 3차시도 Cov1D모델 epoch4  80.38 --->  trainable를 False로 변경\n",
    "4. 4차시도 Cov1D모델 epoch4 73.86 ----> ko.bin을 다시 다운로드\n",
    "5. 5차시도 Cov1D모델 epoch4 74.18 ----> GlobalMaxPooling1D로 모델 변경\n",
    "6. 6차시도 GlobalMaxPooling1D epoch4 73.46 ---> val set과 partial train set 재분배, trainable를 True로 변경, Model LSTM(128)으로 변경, epoch5로 변경\n",
    "7. 7차시도 LSTM(128) epoch5 86.4\n",
    "8. 8차시도 LSTM(8) epoch5 84.81\n",
    "9. 9차시도 Cov1D epoch5 84.7\n",
    "10. 10차시도 Cov1D epoch10 83.97\n",
    "11. 11차시도 Cov1D epoch10 84.62\n",
    "12. 12차시도 GlobalMaxPooling1D epoch20 82.83\n",
    "13. 13차시도 LSTM(32) epoch10 85.52\n",
    "14. 14차시도 LSTM(32) epoch5 85.59   \n",
    "GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys()) # epoch에 따른 그래프를 그려볼 수 있는 항목들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss, 'rx', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "plt.plot(epochs, acc, 'gx', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'g', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-salon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
